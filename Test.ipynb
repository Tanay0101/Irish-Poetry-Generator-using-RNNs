{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense,LSTM,Embedding,Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lyrics.txt\",'r') as file:\n",
    "    data =[]\n",
    "    lines = file.readlines()\n",
    "    tokenizer  = Tokenizer()\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i]\n",
    "        line = line.lower()\n",
    "        data.append(line)\n",
    "        \n",
    "    tokenizer.fit_on_texts(data)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model.h5', custom_objects=None, compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help me Obi Wan Kenobi you are my only hope the the the and the and the of and and the voice and the of and the and the and and the and the and and the and and the and and the and the and and the and the and and the and and the and and the and the and and the and the and and the and and the and and the and the and and the and the and and the and and the and and the and the and and the and the and and the and and the and and the and the and\n"
     ]
    }
   ],
   "source": [
    "max_length = 15\n",
    "index_word = tokenizer.index_word\n",
    "sentence = \"Help me Obi Wan Kenobi you are my only hope\"\n",
    "for i in range(100):\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    sequence = pad_sequences([sequence],maxlen=max_length)\n",
    "    predictions = model.predict([sequence])\n",
    "    prediction = np.argmax(predictions)\n",
    "    word  = index_word.get(prediction)\n",
    "    sentence+=\" \"+word\n",
    "    \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wish I was in India the the the the and the and the and and and the of and the and and the and the and the and the and and the and and the and and the and and the and the and the and and the and and the and and the and and the and the and the and and the and and the and and the and and the and the and the and and the and and the and and the and and the and the and the and and the and and the and and the and and the\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I wish I was in India\"\n",
    "for i in range(100):\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    sequence = pad_sequences([sequence],maxlen=max_length)\n",
    "    predictions = model.predict([sequence])\n",
    "    prediction = np.argmax(predictions)\n",
    "    word  = index_word.get(prediction)\n",
    "    sentence+=\" \"+word\n",
    "    \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May god bless you the the the the the and the and the and the and and and the voice and the and and the and the and the and the and and the and and the and and the and and the and the and the and and the and and the and and the and and the and the and the and and the and and the and and the and and the and the and the and and the and and the and and the and and the and the and the and and the and and the and and the\n"
     ]
    }
   ],
   "source": [
    "sentence = \"May god bless you\"\n",
    "for i in range(100):\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    sequence = pad_sequences([sequence],maxlen=max_length)\n",
    "    predictions = model.predict([sequence])\n",
    "    prediction = np.argmax(predictions)\n",
    "    word  = index_word.get(prediction)\n",
    "    sentence+=\" \"+word\n",
    "    \n",
    "print(sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
